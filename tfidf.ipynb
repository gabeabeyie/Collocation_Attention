{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TOPIC: Beyond News Headlines and TF-IDF: Enhancing Text-Based Forecasting Models with Validated Collocations and Improved Attention.\n",
    "    Author : Gabriel Appau Abeyie.\n",
    "\n",
    "This code is dedicated to determining the apporpriate parameter values for the tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for parsing data\n",
    "import os\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from lxml import etree\n",
    "import sys\n",
    "import glob\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_df(df, ngram_range, min_df=0.0, tfidf_min=0.0, split_ratio=0.625):\n",
    "    # Split the data based on split ratio\n",
    "    split_index = int(len(df) * split_ratio)\n",
    "    df_train = df.iloc[:split_index]\n",
    "    df_test = df.iloc[split_index:]\n",
    "\n",
    "    # Create and fit the TfidfVectorizer\n",
    "    vectorizer_tfidf = TfidfVectorizer(analyzer=\"word\", ngram_range=ngram_range, min_df=min_df, stop_words='english')\n",
    "    X_tfidf_train = vectorizer_tfidf.fit_transform(df_train['cleaned_Text'])\n",
    "\n",
    "    # Calculate total TF-IDF for each n-gram and filter by tfidf_min\n",
    "    columns = vectorizer_tfidf.get_feature_names_out()\n",
    "    df_tfidf_train = pd.DataFrame(X_tfidf_train.toarray(), columns=columns)\n",
    "    df_total_tfidf = df_tfidf_train.sum().reset_index()\n",
    "    df_total_tfidf.columns = ['ngram', 'total_tfidf']\n",
    "    filtered_ngrams = df_total_tfidf[df_total_tfidf['total_tfidf'] >= tfidf_min]['ngram'].tolist()\n",
    "\n",
    "    # Create and fit the CountVectorizer\n",
    "    vectorizer_count = CountVectorizer(vocabulary=filtered_ngrams, ngram_range=ngram_range, stop_words='english')\n",
    "    X_count_train = vectorizer_count.transform(df_train['cleaned_Text'])\n",
    "    X_count_test = vectorizer_count.transform(df_test['cleaned_Text'])\n",
    "\n",
    "    # Convert to dataframes\n",
    "    columns = vectorizer_count.get_feature_names_out()\n",
    "    df_count_train = pd.DataFrame(X_count_train.toarray(), columns=columns, index=df_train.index)\n",
    "    df_count_test = pd.DataFrame(X_count_test.toarray(), columns=columns, index=df_test.index)\n",
    "\n",
    "    df_count_train['Target'] = df['Target'].loc[df_count_train.index]\n",
    "    df_count_test['Target'] = df['Target'].loc[df_count_test.index]\n",
    "\n",
    "    return df_count_train, df_count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "def evaluate_model(df_train, df_test, target_column):\n",
    "    # Separate features and target\n",
    "    X_train = df_train.drop(columns=[target_column])\n",
    "    y_train = df_train[target_column]\n",
    "    X_test = df_test.drop(columns=[target_column])\n",
    "    y_test = df_test[target_column]\n",
    "\n",
    "    # Standardize the features\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(X_train)\n",
    "    X_train = scalar.fit_transform(X_train)\n",
    "    X_test = scalar.transform(X_test)\n",
    "\n",
    "    # Define and train the model\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000, tol=0.001)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Evaluate predictions using your chosen metric (e.g. MSE)\n",
    "    score = -mean_squared_error(y_test, predictions)  # We use a negative sign as grid search considers larger scores as better\n",
    "\n",
    "    return score\n",
    "\n",
    "def grid_search(df, ngram_range, target_column, split_ratio=0.625):\n",
    "    # Define the parameter grid\n",
    "    min_df_values = np.arange(0.01, 0.21, 0.01)  # values from 0.01 to 0.2, step size 0.01\n",
    "    tfidf_min_values = np.arange(0.5, 2.1, 0.1)  # values from 0.5 to 2.0, step size 0.1\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    # Iterate over all combinations of min_df and tfidf_min\n",
    "    for min_df, tfidf_min in product(min_df_values, tfidf_min_values):\n",
    "        # Get train and test dataframes\n",
    "        df_count_train, df_count_test = get_ngram_df(df, ngram_range, min_df, tfidf_min, split_ratio)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        score = evaluate_model(df_count_train, df_count_test, target_column)\n",
    "\n",
    "        # Update best score and best parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = (min_df, tfidf_min)\n",
    "\n",
    "    print(f'Best score: {best_score}')\n",
    "    print(f'Best parameters: min_df={best_params[0]}, tfidf_min={best_params[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nymex = pd.read_csv('E:/Python/Untitled Folder/DCOILWTICO.csv')\n",
    "Target = Nymex[['Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.0010165487015585766\n",
      "Best parameters: min_df=0.060000000000000005, tfidf_min=0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df1, Target['Target']], axis=1)\n",
    "grid_search(df, ngram_range = (1,2), target_column='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.0014909721384406235\n",
      "Best parameters: min_df=0.060000000000000005, tfidf_min=0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "NewsHeadlines = pd.read_csv('E:/Python/Untitled Folder/NewHeadlines.csv')\n",
    "NewsHeadlines = pd.concat([NewsHeadlines, Target['Target']], axis=1)\n",
    "grid_search( NewsHeadlines, ngram_range = (1,1), target_column='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.001584741669566094\n",
      "Best parameters: min_df=0.01, tfidf_min=0.5\n"
     ]
    }
   ],
   "source": [
    "FakeNews = pd.read_csv('E:/Python/Untitled Folder/fake_oil_news_dataset.csv')\n",
    "FakeNews = pd.concat([FakeNews, Target['Target']], axis=1)\n",
    "grid_search(FakeNews, ngram_range = (1,2), target_column='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.0010618200296039272\n",
      "Best parameters: min_df=0.17, tfidf_min=0.6\n"
     ]
    }
   ],
   "source": [
    "grid_search(df, n=3, target_column='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.0013681127563134996\n",
      "Best parameters: min_df=0.09999999999999999, tfidf_min=0.5\n"
     ]
    }
   ],
   "source": [
    "grid_search(df, n=4, target_column='Target')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
