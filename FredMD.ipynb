{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "fred = Fred(api_key='YOUR_API_KEY_HERE')\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\gabeyie\\OneDrive - University of Tennessee\\Documents\\IJF_Paper\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/gabeyie/OneDrive - University of Tennessee/Documents/IJF_Paper\")\n",
    "os.makedirs('Forecasts', exist_ok=True)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price = fred.get_series('DCOILWTICO', observation_start='1986-01-01', observation_end='2020-12-01')\n",
    "monthly_oil = oil_price.resample('ME').mean().ffill()\n",
    "monthly_oil_log = np.log(monthly_oil)\n",
    "monthly_oil_log_diff = monthly_oil_log.diff()\n",
    "monthly_oil_log_diff = monthly_oil_log_diff.dropna()\n",
    "Target = monthly_oil_log_diff.values.ravel()\n",
    "Lag_Target = monthly_oil_log_diff.shift(1)\n",
    "Lag_Target = Lag_Target.bfill()\n",
    "Lag_Target = Lag_Target.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data again due to reset\n",
    "df = pd.read_csv('Data/current.csv', header=0)\n",
    "transformation_codes = df.iloc[0]\n",
    "df = df.drop(df.index[0])  # Remove the transformation codes row\n",
    "df = df.dropna(axis=1)  # Remove columns with \"nan\"\n",
    "\n",
    "# Adjusting transformation_codes to reflect columns actually present after NaN removal\n",
    "transformation_codes = transformation_codes[df.columns]\n",
    "\n",
    "# Define transformation functions again\n",
    "def transform_series(series, code):\n",
    "    if code == 1:\n",
    "        # No transformation\n",
    "        return series\n",
    "    elif code == 2:\n",
    "        # First differences \n",
    "        return series.diff(1)\n",
    "    elif code == 3:\n",
    "        # Second differences\n",
    "        return series.diff(2)\n",
    "    elif code == 4:\n",
    "        # Log transformation\n",
    "        return np.log(series)\n",
    "    elif code == 5:\n",
    "        # Log differences\n",
    "        return np.log(series).diff(1)\n",
    "    elif code == 6:\n",
    "        # Log second differences\n",
    "        return np.log(series).diff(2)\n",
    "    elif code == 7:\n",
    "        # Percent change differences\n",
    "        return series.pct_change()\n",
    "    else:\n",
    "        # Default case, should not be reached\n",
    "        return series\n",
    "\n",
    "# Apply transformations, skipping the 'sasdate' column for transformations\n",
    "for column in df.columns[1:]:  # Exclude date column from transformations\n",
    "    code = int(transformation_codes[column])  # Convert code to integer for processing\n",
    "    df[column] = transform_series(df[column].astype(float), code)\n",
    "\n",
    "# Create lags for all variables except the 'sasdate' column\n",
    "df_lagged = df.copy()\n",
    "\n",
    "# Convert 'sasdate' to datetime format to enable filtering\n",
    "df_lagged['sasdate'] = pd.to_datetime(df_lagged['sasdate'], format='%m/%d/%Y')\n",
    "\n",
    "# Create the sub-dataframe for the specified date range\n",
    "start_date = '1986-01-01'\n",
    "end_date = '2020-12-31'\n",
    "FredMD = df_lagged[(df_lagged['sasdate'] >= start_date) & (df_lagged['sasdate'] <= end_date)]\n",
    "\n",
    "# Set 'sasdate' as the index of the dataframe\n",
    "FredMD.set_index('sasdate', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.625\n",
    "n_samples = len(monthly_oil_log_diff)\n",
    "n_train = int(n_samples * train_ratio)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = monthly_oil_log_diff.iloc[:n_train]\n",
    "test_data = monthly_oil_log_diff.iloc[n_train:]\n",
    "\n",
    "split_index = int(len(FredMD) * train_ratio)\n",
    "FredMD_train = FredMD.iloc[:split_index]\n",
    "FredMD_test = FredMD.iloc[split_index:]\n",
    "\n",
    "# The target variable (values) for training and testing\n",
    "y_train = train_data.values.ravel()\n",
    "y_test = test_data.values.ravel()\n",
    "\n",
    "# Extracting the dates for training and testing\n",
    "train_dates = train_data.index\n",
    "test_dates = test_data.index\n",
    "\n",
    "recessions = fred.get_series('USREC', observation_start='1986-01-01', observation_end='2020-12-01')\n",
    "recessions = recessions.resample('ME').last().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Sahm Rule Recession Indicator\n",
    "sahn_index = fred.get_series('SAHMREALTIME', observation_start='1986-02-01', observation_end='2020-12-01')\n",
    "\n",
    "# Initialize recession_expansion and update as matrices of True\n",
    "recession_expansion = pd.DataFrame(True, index=sahn_index.index, columns=['indicator'])\n",
    "update = recession_expansion.copy()\n",
    "\n",
    "# Update recession_expansion: set to False if Sahm Rule Recession Indicator > 0.5\n",
    "recession_expansion.loc[sahn_index > 0.5, 'indicator'] = False\n",
    "\n",
    "# Update the update matrix: set to True only for the first time point and whenever the value of recession_expansion changes\n",
    "update['indicator'] = recession_expansion['indicator'].ne(recession_expansion['indicator'].shift())\n",
    "update.loc[update.index[0], 'indicator'] = True\n",
    "updates = update.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "sahm_index_train = sahn_index.iloc[:n_train]\n",
    "sahm_index_test = sahn_index.iloc[n_train:]\n",
    "\n",
    "# The target variable (values) for training and testing\n",
    "sahm_index_train = sahm_index_train.values.ravel()\n",
    "sahm_index_test  = sahm_index_test .values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article_Verb_Noun_Noun_Verb_train_combined = pd.read_csv(\"Data/Article_Verb_Noun_Noun_Verb_train_combined.csv\")\n",
    "Article_Verb_Noun_Noun_Verb_test_combined = pd.read_csv(\"Data/Article_Verb_Noun_Noun_Verb_test_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first row of the training sets\n",
    "FredMD_train = FredMD_train.iloc[1:,:]\n",
    "Article_Verb_Noun_Noun_Verb_train_combined  = Article_Verb_Noun_Noun_Verb_train_combined .iloc[1:, :]\n",
    "\n",
    "# Convert all training sets to arrays\n",
    "FredMD_train = np.array(FredMD_train)\n",
    "Article_Verb_Noun_Noun_Verb_train_combined = np.array(Article_Verb_Noun_Noun_Verb_train_combined)\n",
    "\n",
    "# Convert all testing sets to arrays\n",
    "FredMD_test = np.array(FredMD_test)\n",
    "Article_Verb_Noun_Noun_Verb_test_combined = np.array(Article_Verb_Noun_Noun_Verb_test_combined)\n",
    "\n",
    "Text = np.concatenate([Article_Verb_Noun_Noun_Verb_train_combined , Article_Verb_Noun_Noun_Verb_test_combined ])\n",
    "\n",
    "datasets1 = {r'Verb-Noun/Noun-Verb Colls($D_{3,t}$)': (FredMD_train, FredMD_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_factors(data, kmax):\n",
    "    T, N = data.shape\n",
    "    K = min(kmax, N)\n",
    "\n",
    "    xx = (data.T @ data) / (T*N) if N < T else (data @ data.T) / (T*N)\n",
    "\n",
    "    eig_values = np.linalg.eigvals(xx)\n",
    "    d = sorted(eig_values, reverse=True)\n",
    "\n",
    "    ER = [d[k] / d[k+1] for k in range(K-1)]\n",
    "    ER = [0 if np.isnan(e) or np.isinf(e) else e for e in ER]\n",
    "    \n",
    "    n_fac = max(ER)\n",
    "    \n",
    "    num_factors = ER.index(n_fac) + 1 # Remember python indexing starts from 0 so +1\n",
    "\n",
    "    return num_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your objects\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "scaler = StandardScaler()\n",
    "horizons = [1, 3, 6, 9]\n",
    "\n",
    "# Placeholder for storing all values for each horizon and each model\n",
    "predictions_dict_pca = {}\n",
    "y_true_dict_pca = {}\n",
    "\n",
    "# Loop over datasets\n",
    "for model_name, (train, test) in datasets1.items():\n",
    "    predictions_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    y_true_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    # Concatenate train and test\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    # Initialize the model outside the loop\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000000, tol=0.0001)\n",
    "    \n",
    "\n",
    "    # Loop over horizons\n",
    "    for h in horizons:\n",
    "\n",
    "        # Define a variable to keep track of the last observed value of the recession indicator\n",
    "        last_indicator = None\n",
    "        y_true_per_pca_horizon = []\n",
    "        y_pred_per_pca_horizon = []\n",
    "\n",
    "        # Loop over time points in the test set\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "            \n",
    "            # Get train and test data up to the forecast origin\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding lag targets\n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "            \n",
    "            # Check if we should update the model (i.e., if there is a change in the recession_indicator)\n",
    "            current_indicator = updates[i]\n",
    "            if  current_indicator != last_indicator:\n",
    "                # Update the last observed value of the recession indicator\n",
    "                last_indicator = current_indicator\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(train_temp, np.ravel(y_train_temp))\n",
    "\n",
    "                # If no features were selected, refit the model with a different l1_ratio\n",
    "                refit_attempts = 0\n",
    "                while len(np.nonzero(model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                    model = ElasticNet(l1_ratio = 0.05, alpha = 0.05) \n",
    "                    model.fit(train_temp, np.ravel(y_train_temp))\n",
    "                    refit_attempts += 1\n",
    "\n",
    "                #If still no features were selected after 2 attempts, print a warning\n",
    "                if len(np.nonzero(model.coef_)[0]) <= 1:\n",
    "                    print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "\n",
    "            # If any features were selected, apply PCA\n",
    "            if model.coef_.any():\n",
    "                # Get indices of non-zero coefficients\n",
    "                selected_features = np.nonzero(model.coef_)[0]\n",
    "                selected_features_indices = np.nonzero(model.coef_)[0]\n",
    "\n",
    "                # Select the features that were not discarded by the ElasticNet\n",
    "                selected_train_temp = train_temp[:, selected_features]\n",
    "                selected_test_temp = test_temp[:, selected_features]\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(selected_train_temp, kmax=8)  # Choose a suitable value for kmax\n",
    "                pca = PCA(n_components= n_components)\n",
    "                best_pca = pca.fit(selected_train_temp)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                selected_train_temp_pca = best_pca.transform(selected_train_temp)\n",
    "                selected_test_temp_pca = best_pca.transform(selected_test_temp)\n",
    "\n",
    "                 # Add the lagged target as an additional column to the PCA-transformed data\n",
    "                selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp))\n",
    "                selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp))\n",
    "\n",
    "                # Train a linear regression model and compute p-values\n",
    "                lr = LinearRegression()\n",
    "\n",
    "                # Calculate p-values\n",
    "                mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "                fii = mod.fit()\n",
    "                p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "                # Find the significant features\n",
    "                significant_features = p_values[p_values < 0.05].index  # Find features with p-value < 0.05\n",
    "\n",
    "                # Ignore the constant term\n",
    "                significant_features = [i for i in significant_features if i != 'const']\n",
    "\n",
    "                # Create a mapping from column names to indices\n",
    "                column_to_index = {col: idx-1 for idx, col in enumerate(fii.summary2().tables[1].index)}  # idx-1 corrects for the added constant\n",
    "\n",
    "                # Convert column names to indices\n",
    "                significant_indices = [column_to_index[col] for col in significant_features if column_to_index[col] != -1]  # We make sure not to include the constant\n",
    "\n",
    "                # If there are significant features, retrain the model on these\n",
    "                if significant_indices:\n",
    "                    selected_train_temp_pca = selected_train_temp_pca[:, significant_indices]\n",
    "                    selected_test_temp_pca = selected_test_temp_pca[:, significant_indices]\n",
    "                else:\n",
    "                    print(\"No features with p-value < 0.05 was found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "                # Fit the model on the selected (or all) PCA-transformed features\n",
    "                lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "                \n",
    "                # Make a prediction and add it to the predictions list\n",
    "                y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "                y_pred_per_pca_horizon.append(y_pred_pca_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "                # Add true values to a list\n",
    "                y_true_per_pca_horizon.append(y_test_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "        predictions_dict_pca[model_name][h] = y_pred_per_pca_horizon\n",
    "        y_true_dict_pca[model_name][h] = y_true_per_pca_horizon\n",
    "\n",
    "# Save dictionaries to files for future use\n",
    "with open('Forecasts/Sahm_Rule_FredMD_predictions_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_dict_pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for storing all values for each horizon and each model\n",
    "predictions_dict_pca = {}\n",
    "y_true_dict_pca = {}\n",
    "\n",
    "# Loop over datasets\n",
    "for model_name, (train, test) in datasets1.items():\n",
    "    predictions_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    y_true_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    # Concatenate train and test\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    # Initialize the model outside the loop\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000000, tol=0.0001)\n",
    "    text_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000000, tol=0.0001)\n",
    "    \n",
    "\n",
    "    # Loop over horizons\n",
    "    for h in horizons:\n",
    "\n",
    "        # Define a variable to keep track of the last observed value of the recession indicator\n",
    "        last_indicator = None\n",
    "        y_true_per_pca_horizon = []\n",
    "        y_pred_per_pca_horizon = []\n",
    "\n",
    "        # Loop over time points in the test set\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "            \n",
    "            # Get train and test data up to the forecast origin\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            text_train_temp = Text[:i - h + 1]\n",
    "            text_test_temp = Text[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding lag targets\n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Standardize the data\n",
    "            scaler.fit(text_train_temp)\n",
    "            text_train_temp_standardized = scaler.transform(text_train_temp)\n",
    "            text_test_temp_standardized = scaler.transform(text_test_temp)\n",
    "            \n",
    "            # Check if we should update the model (i.e., if there is a change in the recession_indicator)\n",
    "            current_indicator = updates[i]\n",
    "            if  current_indicator != last_indicator:\n",
    "                # Update the last observed value of the recession indicator\n",
    "                last_indicator = current_indicator\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(train_temp, np.ravel(y_train_temp))\n",
    "\n",
    "                # Train the text model on standardized text\n",
    "                text_model.fit(text_train_temp_standardized, np.ravel(y_train_temp))\n",
    "\n",
    "                # If no features were selected, refit the model with a different l1_ratio\n",
    "                refit_attempts = 0\n",
    "                while len(np.nonzero(model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                    model = ElasticNet(l1_ratio = 0.05, alpha = 0.05) \n",
    "                    model.fit(train_temp, np.ravel(y_train_temp))\n",
    "                    refit_attempts += 1\n",
    "\n",
    "                #If still no features were selected after 2 attempts, print a warning\n",
    "                if len(np.nonzero(model.coef_)[0]) <= 1:\n",
    "                    print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "                \n",
    "                # If no features were selected, refit the model with a different l1_ratio\n",
    "                refit_attempts = 0\n",
    "                while len(np.nonzero(text_model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                    text_model = ElasticNet(l1_ratio = 0.1, alpha = 0.1) \n",
    "                    text_model.fit(text_train_temp_standardized, np.ravel(y_train_temp))\n",
    "                    refit_attempts += 1\n",
    "                \n",
    "                #If still no features were selected after 2 attempts, print a warning\n",
    "                if len(np.nonzero(text_model.coef_)[0]) <= 1:\n",
    "                    print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "\n",
    "            # If any features were selected, apply PCA\n",
    "            if model.coef_.any() or text_model.coef_.any():\n",
    "                # Get indices of non-zero coefficients\n",
    "                selected_features = np.nonzero(model.coef_)[0]\n",
    "                text_selected_features = np.nonzero(text_model.coef_)[0]\n",
    "\n",
    "                # Select the features that were not discarded by the ElasticNet\n",
    "                selected_train_temp = train_temp[:, selected_features]\n",
    "                selected_test_temp = test_temp[:, selected_features]\n",
    "\n",
    "                text_selected_train_temp = text_train_temp_standardized[:, text_selected_features]\n",
    "                text_selected_test_temp = text_test_temp_standardized[:, text_selected_features]\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(selected_train_temp, kmax=8)  # Choose a suitable value for kmax\n",
    "                pca = PCA(n_components= n_components)\n",
    "                best_pca = pca.fit(selected_train_temp)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                selected_train_temp_pca = best_pca.transform(selected_train_temp)\n",
    "                selected_test_temp_pca = best_pca.transform(selected_test_temp)\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(text_selected_train_temp, kmax=8)  # Choose a suitable value for kmax\n",
    "                text_pca = PCA(n_components= n_components)\n",
    "                text_best_pca = text_pca.fit(text_selected_train_temp)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                text_selected_train_temp_pca = text_best_pca.transform(text_selected_train_temp)\n",
    "                text_selected_test_temp_pca = text_best_pca.transform(text_selected_test_temp)\n",
    "\n",
    "                 # Add the lagged target as an additional column to the PCA-transformed data\n",
    "                selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp, text_selected_train_temp_pca))\n",
    "                selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp, text_selected_test_temp_pca))\n",
    "\n",
    "                # Train a linear regression model and compute p-values\n",
    "                lr = LinearRegression()\n",
    "\n",
    "                # Calculate p-values\n",
    "                mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "                fii = mod.fit()\n",
    "                p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "                # Find the significant features\n",
    "                significant_features = p_values[p_values < 0.05].index  # Find features with p-value < 0.05\n",
    "\n",
    "                # Ignore the constant term\n",
    "                significant_features = [i for i in significant_features if i != 'const']\n",
    "\n",
    "                # Create a mapping from column names to indices\n",
    "                column_to_index = {col: idx-1 for idx, col in enumerate(fii.summary2().tables[1].index)}  # idx-1 corrects for the added constant\n",
    "\n",
    "                # Convert column names to indices\n",
    "                significant_indices = [column_to_index[col] for col in significant_features if column_to_index[col] != -1]  # We make sure not to include the constant\n",
    "\n",
    "                # If there are significant features, retrain the model on these\n",
    "                if significant_indices:\n",
    "                    selected_train_temp_pca = selected_train_temp_pca[:, significant_indices]\n",
    "                    selected_test_temp_pca = selected_test_temp_pca[:, significant_indices]\n",
    "                else:\n",
    "                    print(\"No features with p-value < 0.05 was found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "                # Fit the model on the selected (or all) PCA-transformed features\n",
    "                lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "                \n",
    "                # Make a prediction and add it to the predictions list\n",
    "                y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "                y_pred_per_pca_horizon.append(y_pred_pca_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "                # Add true values to a list\n",
    "                y_true_per_pca_horizon.append(y_test_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "        predictions_dict_pca[model_name][h] = y_pred_per_pca_horizon\n",
    "        y_true_dict_pca[model_name][h] = y_true_per_pca_horizon\n",
    "\n",
    "# Save dictionaries to files for future use\n",
    "with open('Forecasts/Sahm_Rule_FREDMD_plus_Text_predictions_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_dict_pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for storing all values for each horizon and each model\n",
    "predictions_dict_pca = {}\n",
    "y_true_dict_pca = {}\n",
    "\n",
    "# Loop over datasets\n",
    "for model_name, (train, test) in datasets1.items():\n",
    "    predictions_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    y_true_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    # Concatenate train and test\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    # Initialize the model outside the loop\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000000, tol=0.0001)\n",
    "    \n",
    "\n",
    "    # Loop over horizons\n",
    "    for h in horizons:\n",
    "\n",
    "        # Define a variable to keep track of the last observed value of the recession indicator\n",
    "        y_true_per_pca_horizon = []\n",
    "        y_pred_per_pca_horizon = []\n",
    "\n",
    "        # Loop over time points in the test set\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "            \n",
    "            # Get train and test data up to the forecast origin\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding lag targets\n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "                \n",
    "            # Train the model\n",
    "            model.fit(train_temp, np.ravel(y_train_temp))\n",
    "\n",
    "            # If no features were selected, refit the model with a different l1_ratio\n",
    "            refit_attempts = 0\n",
    "            while len(np.nonzero(model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                model = ElasticNet(l1_ratio = 0.05, alpha = 0.05) \n",
    "                model.fit(train_temp, np.ravel(y_train_temp))\n",
    "                refit_attempts += 1\n",
    "\n",
    "            #If still no features were selected after 2 attempts, print a warning\n",
    "            if len(np.nonzero(model.coef_)[0]) <= 1:\n",
    "                print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "\n",
    "            # If any features were selected, apply PCA\n",
    "            if model.coef_.any():\n",
    "                # Get indices of non-zero coefficients\n",
    "                selected_features = np.nonzero(model.coef_)[0]\n",
    "                selected_features_indices = np.nonzero(model.coef_)[0]\n",
    "\n",
    "                # Select the features that were not discarded by the ElasticNet\n",
    "                selected_train_temp = train_temp[:, selected_features]\n",
    "                selected_test_temp = test_temp[:, selected_features]\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(selected_train_temp, kmax=8)  # Choose a suitable value for kmax\n",
    "                pca = PCA(n_components= n_components)\n",
    "                best_pca = pca.fit(selected_train_temp)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                selected_train_temp_pca = best_pca.transform(selected_train_temp)\n",
    "                selected_test_temp_pca = best_pca.transform(selected_test_temp)\n",
    "\n",
    "                 # Add the lagged target as an additional column to the PCA-transformed data\n",
    "                selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp))\n",
    "                selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp))\n",
    "\n",
    "                # Train a linear regression model and compute p-values\n",
    "                lr = LinearRegression()\n",
    "\n",
    "                # Calculate p-values\n",
    "                mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "                fii = mod.fit()\n",
    "                p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "                # Find the significant features\n",
    "                significant_features = p_values[p_values < 0.05].index  # Find features with p-value < 0.05\n",
    "\n",
    "                # Ignore the constant term\n",
    "                significant_features = [i for i in significant_features if i != 'const']\n",
    "\n",
    "                # Create a mapping from column names to indices\n",
    "                column_to_index = {col: idx-1 for idx, col in enumerate(fii.summary2().tables[1].index)}  # idx-1 corrects for the added constant\n",
    "\n",
    "                # Convert column names to indices\n",
    "                significant_indices = [column_to_index[col] for col in significant_features if column_to_index[col] != -1]  # We make sure not to include the constant\n",
    "\n",
    "                # If there are significant features, retrain the model on these\n",
    "                if significant_indices:\n",
    "                    selected_train_temp_pca = selected_train_temp_pca[:, significant_indices]\n",
    "                    selected_test_temp_pca = selected_test_temp_pca[:, significant_indices]\n",
    "                else:\n",
    "                    print(\"No features with p-value < 0.05 was found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "                # Fit the model on the selected (or all) PCA-transformed features\n",
    "                lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "                \n",
    "                # Make a prediction and add it to the predictions list\n",
    "                y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "                y_pred_per_pca_horizon.append(y_pred_pca_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "                # Add true values to a list\n",
    "                y_true_per_pca_horizon.append(y_test_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "        predictions_dict_pca[model_name][h] = y_pred_per_pca_horizon\n",
    "        y_true_dict_pca[model_name][h] = y_true_per_pca_horizon\n",
    "\n",
    "# Save dictionaries to files for future use\n",
    "with open('Forecasts/Continuous_FREDMD_predictions_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_dict_pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for storing all values for each horizon and each model\n",
    "predictions_dict_pca = {}\n",
    "y_true_dict_pca = {}\n",
    "\n",
    "# Loop over datasets\n",
    "for model_name, (train, test) in datasets1.items():\n",
    "    predictions_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    y_true_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    # Concatenate train and test\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    # Initialize the model outside the loop\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000000, tol=0.0001)\n",
    "    text_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000000, tol=0.0001)\n",
    "    \n",
    "\n",
    "    # Loop over horizons\n",
    "    for h in horizons:\n",
    "\n",
    "        # Define a variable to keep track of the last observed value of the recession indicator\n",
    "        y_true_per_pca_horizon = []\n",
    "        y_pred_per_pca_horizon = []\n",
    "\n",
    "        # Loop over time points in the test set\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "            \n",
    "            # Get train and test data up to the forecast origin\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            text_train_temp = Text[:i - h + 1]\n",
    "            text_test_temp = Text[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding lag targets\n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Standardize the data\n",
    "            scaler.fit(text_train_temp)\n",
    "            text_train_temp_standardized = scaler.transform(text_train_temp)\n",
    "            text_test_temp_standardized = scaler.transform(text_test_temp)\n",
    "                \n",
    "            # Train the model\n",
    "            model.fit(train_temp, np.ravel(y_train_temp))\n",
    "\n",
    "            # Train the text model on standardized text\n",
    "            text_model.fit(text_train_temp_standardized, np.ravel(y_train_temp))\n",
    "\n",
    "            # If no features were selected, refit the model with a different l1_ratio\n",
    "            refit_attempts = 0\n",
    "            while len(np.nonzero(model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                model = ElasticNet(l1_ratio = 0.05, alpha = 0.05) \n",
    "                model.fit(train_temp, np.ravel(y_train_temp))\n",
    "                refit_attempts += 1\n",
    "\n",
    "            #If still no features were selected after 2 attempts, print a warning\n",
    "            if len(np.nonzero(model.coef_)[0]) <= 1:\n",
    "                print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "                \n",
    "            # If no features were selected, refit the model with a different l1_ratio\n",
    "            refit_attempts = 0\n",
    "            while len(np.nonzero(text_model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                text_model = ElasticNet(l1_ratio = 0.1, alpha = 0.1) \n",
    "                text_model.fit(text_train_temp_standardized, np.ravel(y_train_temp))\n",
    "                refit_attempts += 1\n",
    "                \n",
    "            #If still no features were selected after 2 attempts, print a warning\n",
    "            if len(np.nonzero(text_model.coef_)[0]) <= 1:\n",
    "                print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "\n",
    "            # If any features were selected, apply PCA\n",
    "            if model.coef_.any() or text_model.coef_.any():\n",
    "                # Get indices of non-zero coefficients\n",
    "                selected_features = np.nonzero(model.coef_)[0]\n",
    "                text_selected_features = np.nonzero(text_model.coef_)[0]\n",
    "\n",
    "                # Select the features that were not discarded by the ElasticNet\n",
    "                selected_train_temp = train_temp[:, selected_features]\n",
    "                selected_test_temp = test_temp[:, selected_features]\n",
    "\n",
    "                text_selected_train_temp = text_train_temp_standardized[:, text_selected_features]\n",
    "                text_selected_test_temp = text_test_temp_standardized[:, text_selected_features]\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(selected_train_temp, kmax=8)  # Choose a suitable value for kmax\n",
    "                pca = PCA(n_components= n_components)\n",
    "                best_pca = pca.fit(selected_train_temp)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                selected_train_temp_pca = best_pca.transform(selected_train_temp)\n",
    "                selected_test_temp_pca = best_pca.transform(selected_test_temp)\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(text_selected_train_temp, kmax=8)  # Choose a suitable value for kmax\n",
    "                text_pca = PCA(n_components= n_components)\n",
    "                text_best_pca = text_pca.fit(text_selected_train_temp)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                text_selected_train_temp_pca = text_best_pca.transform(text_selected_train_temp)\n",
    "                text_selected_test_temp_pca = text_best_pca.transform(text_selected_test_temp)\n",
    "\n",
    "                 # Add the lagged target as an additional column to the PCA-transformed data\n",
    "                selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp, text_selected_train_temp_pca))\n",
    "                selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp, text_selected_test_temp_pca))\n",
    "\n",
    "                # Train a linear regression model and compute p-values\n",
    "                lr = LinearRegression()\n",
    "\n",
    "                # Calculate p-values\n",
    "                mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "                fii = mod.fit()\n",
    "                p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "                # Find the significant features\n",
    "                significant_features = p_values[p_values < 0.05].index  # Find features with p-value < 0.05\n",
    "\n",
    "                # Ignore the constant term\n",
    "                significant_features = [i for i in significant_features if i != 'const']\n",
    "\n",
    "                # Create a mapping from column names to indices\n",
    "                column_to_index = {col: idx-1 for idx, col in enumerate(fii.summary2().tables[1].index)}  # idx-1 corrects for the added constant\n",
    "\n",
    "                # Convert column names to indices\n",
    "                significant_indices = [column_to_index[col] for col in significant_features if column_to_index[col] != -1]  # We make sure not to include the constant\n",
    "\n",
    "                # If there are significant features, retrain the model on these\n",
    "                if significant_indices:\n",
    "                    selected_train_temp_pca = selected_train_temp_pca[:, significant_indices]\n",
    "                    selected_test_temp_pca = selected_test_temp_pca[:, significant_indices]\n",
    "                else:\n",
    "                    print(\"No features with p-value < 0.05 was found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "                # Fit the model on the selected (or all) PCA-transformed features\n",
    "                lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "                \n",
    "                # Make a prediction and add it to the predictions list\n",
    "                y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "                y_pred_per_pca_horizon.append(y_pred_pca_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "                # Add true values to a list\n",
    "                y_true_per_pca_horizon.append(y_test_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "        predictions_dict_pca[model_name][h] = y_pred_per_pca_horizon\n",
    "        y_true_dict_pca[model_name][h] = y_true_per_pca_horizon\n",
    "\n",
    "# Save dictionaries to files for future use\n",
    "with open('Forecasts/continuous_FREDMD_plus_Text_predictions_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_dict_pca, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
