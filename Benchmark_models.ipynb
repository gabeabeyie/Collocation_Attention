{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import statsmodels.api as sm\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "fred = Fred(api_key='YOUR_API_KEY_HERE')\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\gabeyie\\OneDrive - University of Tennessee\\Documents\\IJF_Paper\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/gabeyie/OneDrive - University of Tennessee/Documents/IJF_Paper\")\n",
    "os.makedirs('Forecasts', exist_ok=True)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"complete_words.json\", \"r\") as file:\n",
    "    complete_words = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price = fred.get_series('DCOILWTICO', observation_start='1986-01-01', observation_end='2020-12-01')\n",
    "monthly_oil = oil_price.resample('ME').mean().ffill()\n",
    "monthly_oil_log = np.log(monthly_oil)\n",
    "monthly_oil_log_diff = monthly_oil_log.diff()\n",
    "monthly_oil_log_diff = monthly_oil_log_diff.dropna()\n",
    "Target = monthly_oil_log_diff.values.ravel()\n",
    "Lag_Target = monthly_oil_log_diff.shift(1)\n",
    "Lag_Target = Lag_Target.bfill()\n",
    "Lag_Target = Lag_Target.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = pd.read_excel('Data/revise.xlsx')\n",
    "Quant_Features = pd.read_excel('Data/EMV_Data.xlsx')\n",
    "\n",
    "# Create the \"Date\" column with yyyy/mm/dd format, setting the day to \"01\"\n",
    "Quant_Features['Date'] = pd.to_datetime(Quant_Features['Year'].astype(str) + '/' + Quant_Features['Month'].astype(str) + '/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quant_Features['Date'] = pd.to_datetime(Quant_Features['Date'])\n",
    "Quant_Features.set_index('Date', inplace=True)\n",
    "\n",
    "# Selecting the desired date range\n",
    "selected_df = Quant_Features.loc['1986-02-01':'2020-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Sahm Rule Recession Indicator\n",
    "sahn_index = fred.get_series('SAHMREALTIME', observation_start='1986-02-01', observation_end='2020-12-01')\n",
    "\n",
    "# Initialize recession_expansion and update as matrices of True\n",
    "recession_expansion = pd.DataFrame(True, index=sahn_index.index, columns=['indicator'])\n",
    "update = recession_expansion.copy()\n",
    "\n",
    "# Update recession_expansion: set to False if Sahm Rule Recession Indicator > 0.5\n",
    "recession_expansion.loc[sahn_index > 0.5, 'indicator'] = False\n",
    "\n",
    "# Update the update matrix: set to True only for the first time point and whenever the value of recession_expansion changes\n",
    "update['indicator'] = recession_expansion['indicator'].ne(recession_expansion['indicator'].shift())\n",
    "update.loc[update.index[0], 'indicator'] = True\n",
    "updates = update.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.625\n",
    "n_samples = len(monthly_oil_log_diff)\n",
    "n_train = int(n_samples * train_ratio)\n",
    "updates = update.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numeric_data = np.array(selected_df)\n",
    "\n",
    "Numeric_data_train = Numeric_data[:n_train]\n",
    "Numeric_data_test = Numeric_data[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store dictionaries\n",
    "dict_list = []\n",
    "\n",
    "# Iterate over rows in the original dataframe\n",
    "for index, row in Features.iterrows():\n",
    "    # Get the text for the current row\n",
    "    text = row['cleaned_Text']\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Initialize a dictionary to hold the counts for the words in your list\n",
    "    count_dict = {}\n",
    "    \n",
    "    # For each word in your list, get its count and add to the dictionary\n",
    "    for word in complete_words:\n",
    "        count_dict[word] = word_counts.get(word, 0)\n",
    "    \n",
    "    # Add the dictionary to the list\n",
    "    dict_list.append(count_dict)\n",
    "\n",
    "# Convert the list of dictionaries into a dataframe\n",
    "df_counts = pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fixed = df_counts\n",
    "split_ratio = 0.625\n",
    "split_index = int(len(Fixed)*split_ratio)\n",
    "Fixed_train = Fixed.iloc[:split_index]\n",
    "Fixed_test = Fixed.iloc[split_index:]\n",
    "Fixed_train = Fixed_train.iloc[1:, :]\n",
    "\n",
    "# Convert all training sets to arrays\n",
    "Fixed_train = np.array(Fixed_train)\n",
    "\n",
    "# Convert all testing sets to arrays\n",
    "Fixed_test = np.array(Fixed_test)\n",
    "\n",
    "\n",
    "Benchmark_datasets1 = {\n",
    "    r'TF-IDF Colls($D_{1,t}$)': (Fixed_train, Fixed_test),\n",
    "    r'Noun-Noun/Adj-Noun Colls($D_{2,t}$)': (Fixed_train, Fixed_test),\n",
    "    r'Verb-Noun/Noun-Verb Colls($D_{3,t}$)': (Fixed_train, Fixed_test),\n",
    "    r'Noun-Adj/Verb-Adj Colls($D_{4,t}$)': (Fixed_train, Fixed_test)      \n",
    "}\n",
    "\n",
    "Benchmark_datasets2 = {\n",
    "    r'TF-IDF Colls($D_{1,t}$)': (Numeric_data_train, Numeric_data_test),\n",
    "    r'Noun-Noun/Adj-Noun Colls($D_{2,t}$)': (Numeric_data_train, Numeric_data_test),\n",
    "    r'Verb-Noun/Noun-Verb Colls($D_{3,t}$)': (Numeric_data_train, Numeric_data_test),\n",
    "    r'Noun-Adj/Verb-Adj Colls($D_{4,t}$)': (Numeric_data_train, Numeric_data_test)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_factors(data, kmax):\n",
    "    T, N = data.shape\n",
    "    K = min(kmax, N)\n",
    "\n",
    "    xx = (data.T @ data) / (T*N) if N < T else (data @ data.T) / (T*N)\n",
    "\n",
    "    eig_values = np.linalg.eigvals(xx)\n",
    "    d = sorted(eig_values, reverse=True)\n",
    "\n",
    "    ER = [d[k] / d[k+1] for k in range(K-1)]\n",
    "    ER = [0 if np.isnan(e) or np.isinf(e) else e for e in ER]\n",
    "    \n",
    "    n_fac = max(ER)\n",
    "    \n",
    "    num_factors = ER.index(n_fac) + 1 # Remember python indexing starts from 0 so +1\n",
    "\n",
    "    return num_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your objects\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "scaler = StandardScaler()\n",
    "horizons = [1, 3, 6, 9]\n",
    "predictions_dict_bm = {}\n",
    "y_true_dict_bm = {}\n",
    "\n",
    "for model_name, (train, test) in Benchmark_datasets1.items():\n",
    "    predictions_dict_bm[model_name] = {h: [] for h in horizons}\n",
    "    y_true_dict_bm[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    for h in horizons:\n",
    "        y_true_per_bm_horizon = []\n",
    "        y_pred_per_bm_horizon = []\n",
    "\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding lag targets\n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "\n",
    "            scaler.fit(train_temp)\n",
    "            train_temp_standardized = scaler.transform(train_temp)\n",
    "            test_temp_standardized = scaler.transform(test_temp)\n",
    "            \n",
    "            n_components = num_factors(train_temp_standardized, kmax=8)\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca.fit(train_temp_standardized)\n",
    "\n",
    "            selected_train_temp_pca = pca.transform(train_temp_standardized)\n",
    "            selected_test_temp_pca = pca.transform(test_temp_standardized)\n",
    "\n",
    "            # Add the lagged target as an additional column to the PCA-transformed data\n",
    "            selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp))\n",
    "            selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp))\n",
    "\n",
    "            lr = LinearRegression()\n",
    "            mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "            fii = mod.fit()\n",
    "            p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "            significant_features = [idx for idx, p in enumerate(p_values[1:]) if p < 0.05]\n",
    "\n",
    "            if significant_features:\n",
    "                selected_train_temp_pca = selected_train_temp_pca[:, significant_features]\n",
    "                selected_test_temp_pca = selected_test_temp_pca[:, significant_features]\n",
    "            else:\n",
    "                print(\"No features with p-value < 0.05 found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "            lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "\n",
    "            # Predict using the freshly fitted model\n",
    "            y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "            y_pred_per_bm_horizon.append(y_pred_pca_temp[h-1])\n",
    "\n",
    "            y_true_per_bm_horizon.append(y_test_temp[h-1])\n",
    "\n",
    "        predictions_dict_bm[model_name][h] = y_pred_per_bm_horizon\n",
    "        y_true_dict_bm[model_name][h] = y_true_per_bm_horizon\n",
    "\n",
    "with open('Forecasts/predictions_dict_bm1.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_dict_bm, f)\n",
    "with open('Forecasts/y_true_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(y_true_dict_bm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for storing all values for each horizon and each model\n",
    "predictions_dict_bm2 = {}\n",
    "y_true_dict_bm2 = {}\n",
    "\n",
    "# Loop over datasets\n",
    "for model_name, (train, test) in Benchmark_datasets2.items():\n",
    "    predictions_dict_bm2[model_name] = {h: [] for h in horizons}\n",
    "    y_true_dict_bm2[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    # Concatenate train and test\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    # Initialize the model outside the loop\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=1000000, tol=0.0001)\n",
    "    \n",
    "    # Define a variable to keep track of the last observed value of the recession indicator\n",
    "    last_indicator = None\n",
    "\n",
    "    # Loop over horizons\n",
    "    for h in horizons:\n",
    "        y_true_per_bm2_horizon = []\n",
    "        y_pred_per_bm2_horizon = []\n",
    "\n",
    "        # Loop over time points in the test set\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "            # Get train and test data up to the forecast origin\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding lag targets\n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Standardize the data\n",
    "            scaler.fit(train_temp)\n",
    "            train_temp_standardized = scaler.transform(train_temp)\n",
    "            test_temp_standardized = scaler.transform(test_temp)\n",
    "            \n",
    "            # Check if we should update the model (i.e., if there is a change in the recession_indicator)\n",
    "            current_indicator = updates[i]\n",
    "            if  current_indicator != last_indicator:\n",
    "                # Update the last observed value of the recession indicator\n",
    "                last_indicator = current_indicator\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(train_temp_standardized, np.ravel(y_train_temp))\n",
    "\n",
    "                # If no features were selected, refit the model with a different l1_ratio\n",
    "                refit_attempts = 0\n",
    "                while len(np.nonzero(model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                    model = ElasticNet(l1_ratio = 0.05, alpha = 0.05)\n",
    "                    model.fit(train_temp_standardized, np.ravel(y_train_temp))\n",
    "                    refit_attempts += 1\n",
    "\n",
    "                #If still no features were selected after 2 attempts, print a warning\n",
    "                if len(np.nonzero(model.coef_)[0]) <= 1:\n",
    "                    print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "\n",
    "            # If any features were selected, apply PCA\n",
    "            if model.coef_.any():\n",
    "                # Get indices of non-zero coefficients\n",
    "                selected_features = np.nonzero(model.coef_)[0]\n",
    "                selected_features_indices = np.nonzero(model.coef_)[0]\n",
    "  \n",
    "                # Select the features that were not discarded by the ElasticNet\n",
    "                selected_train_temp = train_temp[:, selected_features]\n",
    "                selected_test_temp = test_temp[:, selected_features]\n",
    "\n",
    "                # Initialize and fit a new scaler on the selected features\n",
    "                scaler_pca = StandardScaler()\n",
    "                scaler_pca.fit(selected_train_temp)\n",
    "                \n",
    "                # Standardize selected features\n",
    "                selected_train_temp_standardized = scaler_pca.transform(selected_train_temp)\n",
    "                selected_test_temp_standardized = scaler_pca.transform(selected_test_temp)\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(selected_train_temp_standardized, kmax=8)  # Choose a suitable value for kmax\n",
    "                pca = PCA(n_components= n_components)\n",
    "                best_pca = pca.fit(selected_train_temp_standardized)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                selected_train_temp_pca = best_pca.transform(selected_train_temp_standardized)\n",
    "                selected_test_temp_pca = best_pca.transform(selected_test_temp_standardized)\n",
    "\n",
    "                 # Add the lagged target as an additional column to the PCA-transformed data\n",
    "                selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp))\n",
    "                selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp))\n",
    "\n",
    "                # Train a linear regression model and compute p-values\n",
    "                lr = LinearRegression()\n",
    "\n",
    "                # Calculate p-values\n",
    "                mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "                fii = mod.fit()\n",
    "                p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "                # Find the significant features\n",
    "                significant_features = p_values[p_values < 0.05].index  # Find features with p-value < 0.05\n",
    "\n",
    "                # Ignore the constant term\n",
    "                significant_features = [i for i in significant_features if i != 'const']\n",
    "\n",
    "                # Create a mapping from column names to indices\n",
    "                column_to_index = {col: idx-1 for idx, col in enumerate(fii.summary2().tables[1].index)}  # idx-1 corrects for the added constant\n",
    "\n",
    "                # Convert column names to indices\n",
    "                significant_indices = [column_to_index[col] for col in significant_features if column_to_index[col] != -1]  # We make sure not to include the constant\n",
    "\n",
    "                # If there are significant features, retrain the model on these\n",
    "                if significant_indices:\n",
    "                    selected_train_temp_pca = selected_train_temp_pca[:, significant_indices]\n",
    "                    selected_test_temp_pca = selected_test_temp_pca[:, significant_indices]\n",
    "                else:\n",
    "                    print(\"No features with p-value < 0.05 was found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "                # Fit the model on the selected (or all) PCA-transformed features\n",
    "                lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "                \n",
    "                # Make a prediction and add it to the predictions list\n",
    "                y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "                y_pred_per_bm2_horizon.append(y_pred_pca_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "                # Add true values to a list\n",
    "                y_true_per_bm2_horizon.append(y_test_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "        predictions_dict_bm2[model_name][h] = y_pred_per_bm2_horizon\n",
    "        y_true_dict_bm2[model_name][h] = y_true_per_bm2_horizon\n",
    "\n",
    "# Save dictionaries to files for future use\n",
    "with open('Forecasts/predictions_dict_bm2.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions_dict_bm2, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
