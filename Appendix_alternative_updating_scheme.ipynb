{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import statsmodels.api as sm\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "fred = Fred(api_key='YOUR_API_KEY_HERE')\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\gabeyie\\OneDrive - University of Tennessee\\Documents\\IJF_Paper\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/gabeyie/OneDrive - University of Tennessee/Documents/IJF_Paper\")\n",
    "os.makedirs('Forecasts', exist_ok=True)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_price = fred.get_series('DCOILWTICO', observation_start='1986-01-01', observation_end='2020-12-01')\n",
    "monthly_oil = oil_price.resample('ME').mean().ffill()\n",
    "monthly_oil_log = np.log(monthly_oil)\n",
    "monthly_oil_log_diff = monthly_oil_log.diff()\n",
    "monthly_oil_log_diff = monthly_oil_log_diff.dropna()\n",
    "Target = monthly_oil_log_diff.values.ravel()\n",
    "Lag_Target = monthly_oil_log_diff.shift(1)\n",
    "Lag_Target = Lag_Target.bfill()\n",
    "Lag_Target = Lag_Target.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.625\n",
    "n_samples = len(monthly_oil_log_diff)\n",
    "n_train = int(n_samples * train_ratio)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = monthly_oil_log_diff.iloc[:n_train]\n",
    "test_data = monthly_oil_log_diff.iloc[n_train:]\n",
    "\n",
    "# The target variable (values) for training and testing\n",
    "y_train = train_data.values.ravel()\n",
    "y_test = test_data.values.ravel()\n",
    "\n",
    "# Extracting the dates for training and testing\n",
    "train_dates = train_data.index\n",
    "test_dates = test_data.index\n",
    "\n",
    "recessions = fred.get_series('USREC', observation_start='1986-01-01', observation_end='2020-12-01')\n",
    "recessions = recessions.resample('ME').last().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 10th and 90th percentiles for the changes in the log of oil prices\n",
    "lower_bound = monthly_oil_log_diff.quantile(0.10)\n",
    "upper_bound = monthly_oil_log_diff.quantile(0.90)\n",
    "\n",
    "# Initialize recession_expansion and update as matrices of True\n",
    "recession_expansion = pd.DataFrame(True, index=monthly_oil_log_diff.index, columns=['indicator'])\n",
    "update = recession_expansion.copy()\n",
    "\n",
    "# Update recession_expansion based on the significant changes\n",
    "# A change is considered significant if it's below the 10th or above the 90th percentile\n",
    "recession_expansion.loc[(monthly_oil_log_diff <= lower_bound) | (monthly_oil_log_diff >= upper_bound), 'indicator'] = False\n",
    "\n",
    "# Update the 'update' matrix: set to True only for the first time point and whenever the value of recession_expansion changes\n",
    "update['indicator'] = recession_expansion['indicator'].ne(recession_expansion['indicator'].shift())\n",
    "update.loc[update.index[0], 'indicator'] = True # Ensure the first observation is marked for an update\n",
    "\n",
    "# Flatten the 'update' DataFrame to a numpy array for easier processing in the forecasting loop\n",
    "updates = update.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full datasets\n",
    "Article_train = pd.read_csv(\"Data/Article_train.csv\")\n",
    "Article_test = pd.read_csv(\"Data/Article_test.csv\")\n",
    "Headline_train = pd.read_csv(\"Data/Headline_train.csv\")\n",
    "Headline_test = pd.read_csv(\"Data/Headline_test.csv\")\n",
    "\n",
    "# Load combined datasets\n",
    "Article_Noun_Noun_Adjective_Noun_train_combined = pd.read_csv(\"Data/Article_Noun_Noun_Adjective_Noun_train_combined.csv\")\n",
    "Article_Noun_Noun_Adjective_Noun_test_combined = pd.read_csv(\"Data/Article_Noun_Noun_Adjective_Noun_test_combined.csv\")\n",
    "Headline_Noun_Noun_Adjective_Noun_train_combined = pd.read_csv(\"Data/Headline_Noun_Noun_Adjective_Noun_train_combined.csv\")\n",
    "Headline_Noun_Noun_Adjective_Noun_test_combined = pd.read_csv(\"Data/Headline_Noun_Noun_Adjective_Noun_test_combined.csv\")\n",
    "\n",
    "Article_Noun_Adjective_Verb_Adjective_train_combined = pd.read_csv(\"Data/Article_Noun_Adjective_Verb_Adjective_train_combined.csv\")\n",
    "Article_Noun_Adjective_Verb_Adjective_test_combined = pd.read_csv(\"Data/Article_Noun_Adjective_Verb_Adjective_test_combined.csv\")\n",
    "Headline_Noun_Adjective_Verb_Adjective_train_combined = pd.read_csv(\"Data/Headline_Noun_Adjective_Verb_Adjective_train_combined.csv\")\n",
    "Headline_Noun_Adjective_Verb_Adjective_test_combined = pd.read_csv(\"Data/Headline_Noun_Adjective_Verb_Adjective_test_combined.csv\")\n",
    "\n",
    "Article_Verb_Noun_Noun_Verb_train_combined = pd.read_csv(\"Data/Article_Verb_Noun_Noun_Verb_train_combined.csv\")\n",
    "Article_Verb_Noun_Noun_Verb_test_combined = pd.read_csv(\"Data/Article_Verb_Noun_Noun_Verb_test_combined.csv\")\n",
    "Headline_Verb_Noun_Noun_Verb_train_combined = pd.read_csv(\"Data/Headline_Verb_Noun_Noun_Verb_train_combined.csv\")\n",
    "Headline_Verb_Noun_Noun_Verb_test_combined = pd.read_csv(\"Data/Headline_Verb_Noun_Noun_Verb_test_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first row of the training sets\n",
    "Article_train = Article_train.iloc[1:, :]\n",
    "Article_Noun_Noun_Adjective_Noun_train_combined = Article_Noun_Noun_Adjective_Noun_train_combined.iloc[1:, :]\n",
    "Article_Verb_Noun_Noun_Verb_train_combined = Article_Verb_Noun_Noun_Verb_train_combined.iloc[1:, :]\n",
    "Article_Noun_Adjective_Verb_Adjective_train_combined = Article_Noun_Adjective_Verb_Adjective_train_combined.iloc[1:, :]\n",
    "\n",
    "Headline_train = Headline_train.iloc[1:, :]\n",
    "Headline_Noun_Noun_Adjective_Noun_train_combined = Headline_Noun_Noun_Adjective_Noun_train_combined.iloc[1:, :]\n",
    "Headline_Verb_Noun_Noun_Verb_train_combined = Headline_Verb_Noun_Noun_Verb_train_combined.iloc[1:, :]\n",
    "Headline_Noun_Adjective_Verb_Adjective_train_combined = Headline_Noun_Adjective_Verb_Adjective_train_combined.iloc[1:, :]\n",
    "\n",
    "\n",
    "Article_column_names_dict = {\n",
    "    r'TF-IDF Colls($D_{1,t}$)': list(Article_train.columns),      \n",
    "    r'Noun-Noun/Adj-Noun Colls($D_{2,t}$)': list(Article_Noun_Noun_Adjective_Noun_train_combined.columns),\n",
    "    r'Verb-Noun/Noun-Verb Colls($D_{3,t}$)': list(Article_Verb_Noun_Noun_Verb_train_combined.columns),\n",
    "    r'Noun-Adj/Verb-Adj Colls($D_{4,t}$)': list(Article_Noun_Adjective_Verb_Adjective_train_combined.columns)       \n",
    "}\n",
    "\n",
    "Headline_column_names_dict = {\n",
    "    r'TF-IDF Colls($D_{1,t}$)': list(Headline_train.columns),      \n",
    "    r'Noun-Noun/Adj-Noun Colls($D_{2,t}$)': list(Headline_Noun_Noun_Adjective_Noun_train_combined.columns),\n",
    "    r'Verb-Noun/Noun-Verb Colls($D_{3,t}$)': list(Headline_Verb_Noun_Noun_Verb_train_combined.columns),\n",
    "    r'Noun-Adj/Verb-Adj Colls($D_{4,t}$)': list(Headline_Noun_Adjective_Verb_Adjective_train_combined.columns)       \n",
    "}\n",
    "\n",
    "# Convert all training sets to arrays\n",
    "Article_train = np.array(Article_train)\n",
    "Article_Noun_Noun_Adjective_Noun_train_combined = np.array(Article_Noun_Noun_Adjective_Noun_train_combined)\n",
    "Article_Noun_Adjective_Verb_Adjective_train_combined = np.array(Article_Noun_Adjective_Verb_Adjective_train_combined)\n",
    "Article_Verb_Noun_Noun_Verb_train_combined = np.array(Article_Verb_Noun_Noun_Verb_train_combined)\n",
    "\n",
    "Headline_train = np.array(Headline_train)\n",
    "Headline_Noun_Noun_Adjective_Noun_train_combined = np.array(Headline_Noun_Noun_Adjective_Noun_train_combined)\n",
    "Headline_Noun_Adjective_Verb_Adjective_train_combined = np.array(Headline_Noun_Adjective_Verb_Adjective_train_combined)\n",
    "Headline_Verb_Noun_Noun_Verb_train_combined = np.array(Headline_Verb_Noun_Noun_Verb_train_combined)\n",
    "\n",
    "# Convert all testing sets to arrays\n",
    "Article_test = np.array(Article_test)\n",
    "Article_Noun_Noun_Adjective_Noun_test_combined = np.array(Article_Noun_Noun_Adjective_Noun_test_combined)\n",
    "Article_Noun_Adjective_Verb_Adjective_test_combined = np.array(Article_Noun_Adjective_Verb_Adjective_test_combined)\n",
    "Article_Verb_Noun_Noun_Verb_test_combined = np.array(Article_Verb_Noun_Noun_Verb_test_combined)\n",
    "\n",
    "Headline_test = np.array(Headline_test)\n",
    "Headline_Noun_Noun_Adjective_Noun_test_combined = np.array(Headline_Noun_Noun_Adjective_Noun_test_combined)\n",
    "Headline_Noun_Adjective_Verb_Adjective_test_combined = np.array(Headline_Noun_Adjective_Verb_Adjective_test_combined)\n",
    "Headline_Verb_Noun_Noun_Verb_test_combined = np.array(Headline_Verb_Noun_Noun_Verb_test_combined)\n",
    "\n",
    "Article_datasets = {\n",
    "    r'TF-IDF Colls($D_{1,t}$)': (Article_train, Article_test),\n",
    "    r'Noun-Noun/Adj-Noun Colls($D_{2,t}$)': (Article_Noun_Noun_Adjective_Noun_train_combined, Article_Noun_Noun_Adjective_Noun_test_combined),\n",
    "    r'Verb-Noun/Noun-Verb Colls($D_{3,t}$)': (Article_Verb_Noun_Noun_Verb_train_combined, Article_Verb_Noun_Noun_Verb_test_combined),\n",
    "    r'Noun-Adj/Verb-Adj Colls($D_{4,t}$)': (Article_Noun_Adjective_Verb_Adjective_train_combined, Article_Noun_Adjective_Verb_Adjective_test_combined)  \n",
    "}\n",
    "\n",
    "Headline_datasets = {\n",
    "    r'TF-IDF Colls($D_{1,t}$)': (Headline_train, Headline_test),\n",
    "    r'Noun-Noun/Adj-Noun Colls($D_{2,t}$)': (Headline_Noun_Noun_Adjective_Noun_train_combined, Headline_Noun_Noun_Adjective_Noun_test_combined),\n",
    "    r'Verb-Noun/Noun-Verb Colls($D_{3,t}$)': (Headline_Verb_Noun_Noun_Verb_train_combined, Headline_Verb_Noun_Noun_Verb_test_combined),\n",
    "    r'Noun-Adj/Verb-Adj Colls($D_{4,t}$)': (Headline_Noun_Adjective_Verb_Adjective_train_combined, Headline_Noun_Adjective_Verb_Adjective_test_combined)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_factors(data, kmax):\n",
    "    T, N = data.shape\n",
    "    K = min(kmax, N)\n",
    "\n",
    "    xx = (data.T @ data) / (T*N) if N < T else (data @ data.T) / (T*N)\n",
    "\n",
    "    eig_values = np.linalg.eigvals(xx)\n",
    "    d = sorted(eig_values, reverse=True)\n",
    "\n",
    "    ER = [d[k] / d[k+1] for k in range(K-1)]\n",
    "    ER = [0 if np.isnan(e) or np.isinf(e) else e for e in ER]\n",
    "    \n",
    "    n_fac = max(ER)\n",
    "    \n",
    "    num_factors = ER.index(n_fac) + 1 # Remember python indexing starts from 0 so +1\n",
    "\n",
    "    return num_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your objects\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "scaler = StandardScaler()\n",
    "horizons = [1, 3, 6, 9]\n",
    "\n",
    "# Placeholder for storing all values for each horizon and each model\n",
    "Article_predictions_dict_pca = {}\n",
    "Article_y_true_dict_pca = {}\n",
    "Article_elasticnet_feature_counts = {}\n",
    "Article_elasticnet_selected_indices = {model_name: {h: [] for h in horizons} for model_name, _ in Article_datasets.items()}\n",
    "Article_pca_components_counts = {model_name: {h: [] for h in horizons} for model_name, _ in Article_datasets.items()}\n",
    "\n",
    "# Loop over datasets\n",
    "for model_name, (train, test) in Article_datasets.items():\n",
    "    Article_predictions_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    Article_y_true_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    Article_elasticnet_feature_counts[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    # Concatenate train and test\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    # Initialize the model outside the loop\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=100000, tol=0.0001)\n",
    "\n",
    "    # Loop over horizons\n",
    "    for h in horizons:\n",
    "        # Define a variable to keep track of the last observed value of the recession indicator\n",
    "        last_indicator = None\n",
    "        y_true_per_pca_horizon = []\n",
    "        y_pred_per_pca_horizon = []\n",
    "\n",
    "        # Loop over time points in the test set\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "            # Get train and test data up to the forecast origin\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            \n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Standardize the data\n",
    "            scaler.fit(train_temp)\n",
    "            train_temp_standardized = scaler.transform(train_temp)\n",
    "            test_temp_standardized = scaler.transform(test_temp)\n",
    "            \n",
    "            # Check if we should update the model (i.e., if there is a change in the recession_indicator)\n",
    "            current_indicator = updates[i]\n",
    "            if  current_indicator != last_indicator:\n",
    "                # Update the last observed value of the recession indicator\n",
    "                last_indicator = current_indicator\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(train_temp_standardized, np.ravel(y_train_temp))\n",
    "\n",
    "                # If no features were selected, refit the model with a different l1_ratio\n",
    "                refit_attempts = 0\n",
    "                while len(np.nonzero(model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                    model = ElasticNet(l1_ratio = 0.1, alpha = 0.1) \n",
    "                    model.fit(train_temp_standardized, np.ravel(y_train_temp))\n",
    "                    refit_attempts += 1\n",
    "\n",
    "                #If still no features were selected after 2 attempts, print a warning\n",
    "                if len(np.nonzero(model.coef_)[0]) <= 1:\n",
    "                    print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "\n",
    "            # If any features were selected, apply PCA\n",
    "            if model.coef_.any():\n",
    "                # Get indices of non-zero coefficients\n",
    "                selected_features = np.nonzero(model.coef_)[0]\n",
    "                selected_features_indices = np.nonzero(model.coef_)[0]\n",
    "\n",
    "                # Track number of features selected by ElasticNetCV\n",
    "                selected_features_count = len(np.nonzero(model.coef_)[0])\n",
    "                Article_elasticnet_feature_counts[model_name][h].append(selected_features_count)\n",
    "                Article_elasticnet_selected_indices[model_name][h].append(selected_features_indices)\n",
    "  \n",
    "                # Select the features that were not discarded by the ElasticNet\n",
    "                selected_train_temp = train_temp[:, selected_features]\n",
    "                selected_test_temp = test_temp[:, selected_features]\n",
    "\n",
    "                # Initialize and fit a new scaler on the selected features\n",
    "                scaler_pca = StandardScaler()\n",
    "                scaler_pca.fit(selected_train_temp)\n",
    "                \n",
    "                # Standardize selected features\n",
    "                selected_train_temp_standardized = scaler_pca.transform(selected_train_temp)\n",
    "                selected_test_temp_standardized = scaler_pca.transform(selected_test_temp)\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(selected_train_temp_standardized, kmax=8)  # Choose a suitable value for kmax\n",
    "                pca = PCA(n_components= n_components)\n",
    "                best_pca = pca.fit(selected_train_temp_standardized)\n",
    "\n",
    "                Article_pca_components_counts[model_name][h].append(n_components)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                selected_train_temp_pca = best_pca.transform(selected_train_temp_standardized)\n",
    "                selected_test_temp_pca = best_pca.transform(selected_test_temp_standardized)\n",
    "\n",
    "                 # Add the lagged target as an additional column to the PCA-transformed data\n",
    "                selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp))\n",
    "                selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp))\n",
    "\n",
    "                # Train a linear regression model and compute p-values\n",
    "                lr = LinearRegression()\n",
    "\n",
    "                # Calculate p-values\n",
    "                mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "                fii = mod.fit()\n",
    "                p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "                # Find the significant features\n",
    "                significant_features = p_values[p_values < 0.05].index  # Find features with p-value < 0.05\n",
    "\n",
    "                # Ignore the constant term\n",
    "                significant_features = [i for i in significant_features if i != 'const']\n",
    "\n",
    "                # Create a mapping from column names to indices\n",
    "                column_to_index = {col: idx-1 for idx, col in enumerate(fii.summary2().tables[1].index)}  # idx-1 corrects for the added constant\n",
    "\n",
    "                # Convert column names to indices\n",
    "                significant_indices = [column_to_index[col] for col in significant_features if column_to_index[col] != -1]  # We make sure not to include the constant\n",
    "\n",
    "                # If there are significant features, retrain the model on these\n",
    "                if significant_indices:\n",
    "                    selected_train_temp_pca = selected_train_temp_pca[:, significant_indices]\n",
    "                    selected_test_temp_pca = selected_test_temp_pca[:, significant_indices]\n",
    "                else:\n",
    "                    print(\"No features with p-value < 0.05 was found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "                # Fit the model on the selected (or all) PCA-transformed features\n",
    "                lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "                \n",
    "                # Make a prediction and add it to the predictions list\n",
    "                y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "                y_pred_per_pca_horizon.append(y_pred_pca_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "                # Add true values to a list\n",
    "                y_true_per_pca_horizon.append(y_test_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "        Article_predictions_dict_pca[model_name][h] = y_pred_per_pca_horizon\n",
    "        Article_y_true_dict_pca[model_name][h] = y_true_per_pca_horizon\n",
    "\n",
    "# Save dictionaries to files for future use\n",
    "with open('Forecasts/Alt_Updating_Scheme_Article_predictions_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(Article_predictions_dict_pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for storing all values for each horizon and each model\n",
    "Headline_predictions_dict_pca = {}\n",
    "Headline_y_true_dict_pca = {}\n",
    "Headline_elasticnet_feature_counts = {}\n",
    "Headline_elasticnet_selected_indices = {model_name: {h: [] for h in horizons} for model_name, _ in Headline_datasets.items()}\n",
    "Headline_pca_components_counts = {model_name: {h: [] for h in horizons} for model_name, _ in Headline_datasets.items()}\n",
    "\n",
    "# Loop over datasets\n",
    "for model_name, (train, test) in Headline_datasets.items():\n",
    "    Headline_predictions_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    Headline_y_true_dict_pca[model_name] = {h: [] for h in horizons}\n",
    "    Headline_elasticnet_feature_counts[model_name] = {h: [] for h in horizons}\n",
    "\n",
    "    # Concatenate train and test\n",
    "    data = np.concatenate([train, test])\n",
    "\n",
    "    # Initialize the model outside the loop\n",
    "    model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=tscv, max_iter=100000, tol=0.0001)\n",
    "\n",
    "    # Loop over horizons\n",
    "    for h in horizons:\n",
    "        # Define a variable to keep track of the last observed value of the recession indicator\n",
    "        last_indicator = None\n",
    "        y_true_per_pca_horizon = []\n",
    "        y_pred_per_pca_horizon = []\n",
    "\n",
    "        # Loop over time points in the test set\n",
    "        for i in range(len(train) + h - 1, len(data)):\n",
    "            # Get train and test data up to the forecast origin\n",
    "            train_temp = data[:i - h + 1]\n",
    "            test_temp = data[i - h + 1:i + 1]\n",
    "\n",
    "            # Get the corresponding targets\n",
    "            y_train_temp = Target[:i - h + 1]\n",
    "            y_test_temp = Target[i - h + 1:i + 1]\n",
    "\n",
    "            \n",
    "            y_lag_train_temp = Lag_Target[:i - h + 1]\n",
    "            y_lag_test_temp = Lag_Target[i - h + 1:i + 1]\n",
    "\n",
    "            # Standardize the data\n",
    "            scaler.fit(train_temp)\n",
    "            train_temp_standardized = scaler.transform(train_temp)\n",
    "            test_temp_standardized = scaler.transform(test_temp)\n",
    "            \n",
    "            # Check if we should update the model (i.e., if there is a change in the recession_indicator)\n",
    "            current_indicator = updates[i]\n",
    "            if  current_indicator != last_indicator:\n",
    "                # Update the last observed value of the recession indicator\n",
    "                last_indicator = current_indicator\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(train_temp_standardized, np.ravel(y_train_temp))\n",
    "\n",
    "                # If no features were selected, refit the model with a different l1_ratio\n",
    "                refit_attempts = 0\n",
    "                while len(np.nonzero(model.coef_)[0]) <= 1 and refit_attempts < 2:\n",
    "                    model = ElasticNet(l1_ratio = 0.1, alpha = 0.1) \n",
    "                    model.fit(train_temp_standardized, np.ravel(y_train_temp))\n",
    "                    refit_attempts += 1\n",
    "\n",
    "                #If still no features were selected after 2 attempts, print a warning\n",
    "                if len(np.nonzero(model.coef_)[0]) <= 1:\n",
    "                    print(f'Warning: Model failed to select more than one feature after {refit_attempts} attempts.')\n",
    "\n",
    "            # If any features were selected, apply PCA\n",
    "            if model.coef_.any():\n",
    "                # Get indices of non-zero coefficients\n",
    "                selected_features = np.nonzero(model.coef_)[0]\n",
    "                selected_features_indices = np.nonzero(model.coef_)[0]\n",
    "\n",
    "                # Track number of features selected by ElasticNetCV\n",
    "                selected_features_count = len(np.nonzero(model.coef_)[0])\n",
    "                Headline_elasticnet_feature_counts[model_name][h].append(selected_features_count)\n",
    "                Headline_elasticnet_selected_indices[model_name][h].append(selected_features_indices)\n",
    "  \n",
    "                # Select the features that were not discarded by the ElasticNet\n",
    "                selected_train_temp = train_temp[:, selected_features]\n",
    "                selected_test_temp = test_temp[:, selected_features]\n",
    "\n",
    "                # Initialize and fit a new scaler on the selected features\n",
    "                scaler_pca = StandardScaler()\n",
    "                scaler_pca.fit(selected_train_temp)\n",
    "                \n",
    "                # Standardize selected features\n",
    "                selected_train_temp_standardized = scaler_pca.transform(selected_train_temp)\n",
    "                selected_test_temp_standardized = scaler_pca.transform(selected_test_temp)\n",
    "\n",
    "                # Define PCA\n",
    "                n_components = num_factors(selected_train_temp_standardized, kmax=8)  # Choose a suitable value for kmax\n",
    "                pca = PCA(n_components= n_components)\n",
    "                best_pca = pca.fit(selected_train_temp_standardized)\n",
    "\n",
    "                Headline_pca_components_counts[model_name][h].append(n_components)\n",
    "\n",
    "                # Transform data using the best PCA\n",
    "                selected_train_temp_pca = best_pca.transform(selected_train_temp_standardized)\n",
    "                selected_test_temp_pca = best_pca.transform(selected_test_temp_standardized)\n",
    "\n",
    "                 # Add the lagged target as an additional column to the PCA-transformed data\n",
    "                selected_train_temp_pca = np.column_stack((selected_train_temp_pca, y_lag_train_temp))\n",
    "                selected_test_temp_pca = np.column_stack((selected_test_temp_pca, y_lag_test_temp))\n",
    "\n",
    "                # Train a linear regression model and compute p-values\n",
    "                lr = LinearRegression()\n",
    "\n",
    "                # Calculate p-values\n",
    "                mod = sm.OLS(np.ravel(y_train_temp), sm.add_constant(selected_train_temp_pca))\n",
    "                fii = mod.fit()\n",
    "                p_values = fii.summary2().tables[1]['P>|t|']\n",
    "\n",
    "                # Find the significant features\n",
    "                significant_features = p_values[p_values < 0.05].index  # Find features with p-value < 0.05\n",
    "\n",
    "                # Ignore the constant term\n",
    "                significant_features = [i for i in significant_features if i != 'const']\n",
    "\n",
    "                # Create a mapping from column names to indices\n",
    "                column_to_index = {col: idx-1 for idx, col in enumerate(fii.summary2().tables[1].index)}  # idx-1 corrects for the added constant\n",
    "\n",
    "                # Convert column names to indices\n",
    "                significant_indices = [column_to_index[col] for col in significant_features if column_to_index[col] != -1]  # We make sure not to include the constant\n",
    "\n",
    "                # If there are significant features, retrain the model on these\n",
    "                if significant_indices:\n",
    "                    selected_train_temp_pca = selected_train_temp_pca[:, significant_indices]\n",
    "                    selected_test_temp_pca = selected_test_temp_pca[:, significant_indices]\n",
    "                else:\n",
    "                    print(\"No features with p-value < 0.05 was found. Retaining all PCA-transformed features.\")\n",
    "\n",
    "                # Fit the model on the selected (or all) PCA-transformed features\n",
    "                lr.fit(selected_train_temp_pca, np.ravel(y_train_temp))\n",
    "                \n",
    "                # Make a prediction and add it to the predictions list\n",
    "                y_pred_pca_temp = lr.predict(selected_test_temp_pca)\n",
    "                y_pred_per_pca_horizon.append(y_pred_pca_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "                # Add true values to a list\n",
    "                y_true_per_pca_horizon.append(y_test_temp[h-1]) # Remember python indexing starts from 0\n",
    "\n",
    "        Headline_predictions_dict_pca[model_name][h] = y_pred_per_pca_horizon\n",
    "        Headline_y_true_dict_pca[model_name][h] = y_true_per_pca_horizon\n",
    "\n",
    "# Save dictionaries to files for future use\n",
    "with open('Forecasts/Alt_Updating_Scheme_Headline_predictions_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(Headline_predictions_dict_pca, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold feature selection frequencies for each model\n",
    "Article_feature_selection_frequencies = {model_name: defaultdict(int) for model_name in Article_datasets.keys()}\n",
    "Headline_feature_selection_frequencies = {model_name: defaultdict(int) for model_name in Headline_datasets.keys()}\n",
    "\n",
    "# Loop over each model, horizon, and selected feature indices to count frequencies\n",
    "for model_name, horizons_data in Article_elasticnet_selected_indices.items():\n",
    "    for horizon, indices_list in horizons_data.items():\n",
    "        for indices in indices_list:\n",
    "            for idx in indices:\n",
    "                Article_feature_selection_frequencies[model_name][idx] += 1\n",
    "                \n",
    "# Loop over each model, horizon, and selected feature indices to count frequencies\n",
    "for model_name, horizons_data in Headline_elasticnet_selected_indices.items():\n",
    "    for horizon, indices_list in horizons_data.items():\n",
    "        for indices in indices_list:\n",
    "            for idx in indices:\n",
    "                Headline_feature_selection_frequencies[model_name][idx] += 1\n",
    "                \n",
    "# Initialize the dictionary for top features per model\n",
    "Article_top_features_per_model = {}\n",
    "Headline_top_features_per_model = {}\n",
    "\n",
    "N = 20  # Number of top features to identify\n",
    "\n",
    "for model_name, frequencies in Article_feature_selection_frequencies.items():\n",
    "    # Retrieve the list of original feature names for the current model from your dictionary\n",
    "    feature_names = Article_column_names_dict[model_name]\n",
    "    \n",
    "    # Sort the features by their selection frequency, in descending order, and pick the top N\n",
    "    sorted_features = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "    \n",
    "    # Map the indices of the top features to their actual names using the list from 'column_names_dict'\n",
    "    Article_top_features_per_model[model_name] = [(feature_names[idx], freq) for idx, freq in sorted_features]\n",
    "    \n",
    "for model_name, frequencies in Headline_feature_selection_frequencies.items():\n",
    "    # Retrieve the list of original feature names for the current model from your dictionary\n",
    "    feature_names = Headline_column_names_dict[model_name]\n",
    "    \n",
    "    # Sort the features by their selection frequency, in descending order, and pick the top N\n",
    "    sorted_features = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "    \n",
    "    # Map the indices of the top features to their actual names using the list from 'column_names_dict'\n",
    "    Headline_top_features_per_model[model_name] = [(feature_names[idx], freq) for idx, freq in sorted_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save other key variables (test dates, horizons, update indicators, etc.)\n",
    "with open(\"Forecasts/Alt_Updating_Scheme_Variables.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"Alt_Updating_Scheme_Article_elasticnet_feature_counts\": Article_elasticnet_feature_counts,\n",
    "        \"Alt_Updating_Scheme_Headline_elasticnet_feature_counts\": Headline_elasticnet_feature_counts,\n",
    "        \"Alt_Updating_Scheme_Article_pca_components_counts\": Article_pca_components_counts,\n",
    "        \"Alt_Updating_Scheme_Headline_pca_components_counts\": Headline_pca_components_counts,\n",
    "        \"Alt_Updating_Scheme_Article_column_names_dict\": Article_column_names_dict,\n",
    "        \"Alt_Updating_Scheme_Headline_column_names_dict\": Headline_column_names_dict,\n",
    "        \"Alt_Updating_Scheme_Article_top_features_per_model\": Article_top_features_per_model,\n",
    "        \"Alt_Updating_Scheme_Headline_top_features_per_model\": Headline_top_features_per_model,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Forecasts/Alt_Updating_Scheme_other_variables.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"test_dates\": test_dates,\n",
    "        \"horizons\": horizons,\n",
    "        \"update_indicator\": update['indicator'],\n",
    "        \"recessions\": recessions,\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
